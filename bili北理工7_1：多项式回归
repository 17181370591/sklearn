
import numpy as np,pickle,time,pandas as pd,sklearn
from sklearn import datasets,metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split,cross_val_score
import matplotlib.pyplot as plt,cv2
from sklearn.cluster import KMeans,DBSCAN
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import Imputer,PolynomialFeatures
from sklearn.metrics import classification_report




a=pd.read_csv('2.txt',header=None)
x,y=a.loc[:,0].values.reshape(-1,1),a.loc[:,1].values.reshape(-1,1)
X=np.arange(x.min(),x.max()).reshape(-1,1)



poly_reg=PolynomialFeatures(degree=2)         #degree是多项式最高次数
x_poly=poly_reg.fit_transform(x)
'''
这说明poly_reg.fit_transform(x)就是将x代入ax**2+b*x+c，然后进行线性回归计算abc
>>> poly_reg.fit_transform([[3]])
array([[1., 3., 9.]])
'''


line=sklearn.linear_model.LinearRegression()
line.fit(x_poly,y)
print(line.coef_,line.intercept_)


plt.scatter(x,y,color='r')
plt.plot(X,line.predict(poly_reg.fit_transform(X)),color='b')
plt.show()


'''
========================================================================
另一个例子和另一种写法
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np
from matplotlib import pyplot as plt

model = Pipeline([('poly', PolynomialFeatures(degree=3)),
                  ('linear', LinearRegression(fit_intercept=False))])

x = np.arange(5)                            #>>>array([0, 1, 2, 3, 4])
y = 3 - 2 * x + x ** 2 - x ** 3

model = model.fit(x[:, np.newaxis], y)
print(model.named_steps['linear'].coef_)
#>>>[ 3. -2.  1. -1.]


p=PolynomialFeatures(degree=3)
z=p.fit_transform(x[:,None])

l=LinearRegression()
l.fit(z,y)

print(l.coef_,l.intercept_)
#>>>[ 0. -2.  1. -1.]     2.999999999999993

'''
