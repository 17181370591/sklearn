#http://sklearn.apachecn.org/cn/0.19.0/modules/svm.html
'''
经常用到sklearn中的SVC函数，这里把文档中的参数翻译了一些，以备不时之需。

本身这个函数也是基于libsvm实现的，所以在参数设置上有很多相似的地方。（PS: libsvm中的二次规划问题的解决算法是SMO）。
sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True,
      probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False, 
      max_iter=-1, decision_function_shape=None,random_state=None)

参数：

l  C：C-SVC的惩罚参数C?默认值是1.0

C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，
这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。

l  kernel ：核函数，默认是rbf，可以是‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 

  　　0 – 线性：u'v

 　　 1 – 多项式：(gamma*u'*v + coef0)^degree

  　　2 – RBF函数：exp(-gamma|u-v|^2)

  　　3 –sigmoid：tanh(gamma*u'*v + coef0)

l  degree ：多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。

l  gamma ： ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features

l  coef0 ：核函数的常数项。对于‘poly’和 ‘sigmoid’有用。

l  probability ：是否采用概率估计？.默认为False

l  shrinking ：是否采用shrinking heuristic方法，默认为true

l  tol ：停止训练的误差值大小，默认为1e-3

l  cache_size ：核函数cache缓存大小，默认为200

l  class_weight ：类别的权重，字典形式传递。设置第几类的参数C为weight*C(C-SVC中的C)

l  verbose ：允许冗余输出？

l  max_iter ：最大迭代次数。-1为无限制。

l  decision_function_shape ：‘ovo’, ‘ovr’ or None, default=None3

l  random_state ：数据洗牌时的种子值，int值

主要调节的参数有：C、kernel、degree、gamma、coef0。

======================================================================
#https://www.cnblogs.com/xiaotan-code/p/6700290.html

SVC继承了父类BaseSVC

SVC类主要方法：

★__init__() 主要参数：

C: float参数 默认值为1.0

错误项的惩罚系数。C越大，即对分错样本的惩罚程度越大，因此在训练样本中准确率越高，但是泛化能力降低，
也就是对测试数据的分类准确率降低。相反，减小C的话，容许训练样本中有一些误分类错误样本，泛化能力强。
对于训练样本带有噪声的情况，一般采用后者，把训练样本集中错误分类的样本作为噪声。

kernel: str参数 默认为‘rbf’

算法中采用的核函数类型，可选参数有：

‘linear’:线性核函数

‘poly’：多项式核函数

‘rbf’：径像核函数/高斯核

‘sigmod’:sigmod核函数

‘precomputed’:核矩阵

具体这些核函数类型，请参考上一篇博客中的核函数。需要说明的是，precomputed表示自己提前计算好核函数矩阵，
这时候算法内部就不再用核函数去计算核矩阵，而是直接用你给的核矩阵。核矩阵为如下形式：



还有一点需要说明，除了上面限定的核函数外，还可以给出自己定义的核函数，其实内部就是用你自己定义的核函数来计算核矩阵。

degree:int型参数 默认为3

这个参数只对多项式核函数有用，是指多项式核函数的阶数n

如果给的核函数参数是其他核函数，则会自动忽略该参数。

gamma：float参数 默认为auto

核函数系数，只对‘rbf’,‘poly’,‘sigmod’有效。

如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features.

coef0:float参数 默认为0.0

核函数中的独立项，只有对‘poly’和‘sigmod’核函数有用，是指其中的参数c

probability：bool参数 默认为False

是否启用概率估计。 这必须在调用fit()之前启用，并且会fit()方法速度变慢。

shrinking：bool参数 默认为True

是否采用启发式收缩方式

tol: float参数  默认为1e^-3

svm停止训练的误差精度

cache_size：float参数 默认为200

指定训练所需要的内存，以MB为单位，默认为200MB。

class_weight：字典类型或者‘balance’字符串。默认为None

给每个类别分别设置不同的惩罚参数C，如果没有给，则会给所有类别都给C=1，即前面参数指出的参数C.

如果给定参数‘balance’，则使用y的值自动调整与输入数据中的类频率成反比的权重。

verbose ：bool参数 默认为False

是否启用详细输出。 此设置利用libsvm中的每个进程运行时设置，如果启用，可能无法在多线程上下文中正常工作。
一般情况都设为False，不用管它。

max_iter ：int参数 默认为-1

最大迭代次数，如果为-1，表示不限制

random_state：int型参数 默认为None

伪随机数发生器的种子,在混洗数据时用于概率估计。

★fit()方法：用于训练SVM，具体参数已经在定义SVC对象的时候给出了，这时候只需要给出数据集X和X对应的标签y即可。

★predict()方法：基于以上的训练，对预测样本T进行类别预测，因此只需要接收一个测试集T，
该函数返回一个数组表示个测试样本的类别。

★属性有哪些：

svc.n_support_：各类各有多少个支持向量

svc.support_：各类的支持向量在训练样本中的索引

svc.support_vectors_：各类所有的支持向量
'''











from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np
from matplotlib import pyplot as plt
from sklearn.svm import SVC



x=np.array([[1,1],[1,2],[1,3],[1,4],[2,1],[2,2],[3,1],
            [4,1],[5,1],[5,2],[6,1],[6,2],[6,3],[6,4],
            [3,3],[3,4],[3,5],[4,3],[4,4],[4,5]])
y=np.array([1]*14+[-1]*6)
T=np.array([[0.5,0.5],[1.5,1.5],[3.5,3.5],[4,5.5]])



#svc=SVC(kernel='linear',class_weight={-1:7})
#这里设置class_weight将-1的C设为7倍（原来1现在就是7），增加-1分类错误的惩罚，会使边界点更多的偏向1

#svc=SVC(）
#svc=SVC(kernel='poly',degree=2,gamma=1,coef0=0)
svc=SVC(kernel='linear')
svc.fit(x,y)

'''
线性核的结果：
>>> svc.coef_
array([[-7.10542736e-15, -6.66425158e-01]])
>>> svc.intercept_
array([2.33248805])
'''

r=[1,1,-1,-1]
print(svc.score(T,r))                                 #1
print(svc.score(T,[1,1,1,-1]))                        #0.75
z=svc.predict(T)

plt.scatter(x[:,0][:,None],x[:,1],color='r')
plt.scatter(T[:,0],z,color='b')
plt.scatter(T[:,0],T[:,1],color='g')
plt.show()
